{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Task - Exploratory Data Analysis\n",
       "\n",
       "#### Aim\n",
       "\n",
       "Exploratory Data Analysis is intended to generate insights for subsequent analysis.\n",
       "\n",
       "As such, its aims are more diffuse. For the Instacart set, much of its purpose is to\n",
       "create new dataframes by joining the existing tables and looking at typical data, its\n",
       "cardinality, etc.\n",
       "\n",
       "#### Detail\n",
       "\n",
       "Kieran has created a [notebook](Assignment_2_-Setup.ipynb) that shows how to create\n",
       "your own slices of data from the Instacart data set. It includes settings\n",
       "like `MAX_PRODUCTS` and `MAX_USERS`, so that is is possible to select a slice that is\n",
       "specific to you and which is sized appropriately for your laptop.\n",
       "\n",
       "Regarding EDA, the Python notebooks that Kieran provided for the Week 8 (ARM) Practicals provide an\n",
       "excellent basis for your own explorations. In particular, [Practical\n",
       "C](https://kmurphy.bitbucket.io/modules/Data_Mining/topics/08-Association_Analysis/13-Practical_C_-_ARM_Instacart_Dataset/files/Practical_C_-_ARM_Instacart_Dataset.ipynb)\n",
       "and parts of Practicals A and B should be used as a starting point for your own\n",
       "investigations.\n",
       "\n",
       "For the classification task, Kieran has also studied reorder rate as a function\n",
       "of \"order added to cart\", see ![Reorder-rate vs Order-added-to-cart](add_order_v_reorder_rate.png).\n",
       "\n",
       "We recommend that you recreate his analysis and use linear regression to derive the\n",
       "relationship between reorder rate and the order in which products are added to each cart,\n",
       "as this represents a feature that might prove helpful when predicting the product mix\n",
       "of the next order (which is the subject of the Classification task).\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Task - Association Rule Mining \n",
       "\n",
       "#### Aim\n",
       "\n",
       "Perform an association rule mining (ARM) analysis (of your slice) of the Instacart dataset.\n",
       "\n",
       "#### Detail\n",
       "\n",
       "Perform ARM using __product__, __aisle__, and __department__ as the item.\n",
       "\n",
       "To perform an ARM with item=product we need to reduce the number of products (from the initial 50K products). A crude approach is to simply drop all product which appear less than some minimum frequency threshold (say __PRODUCT_KEEP_MIN_FREQ=500__).\n",
       "\n",
       "The table __orders_products__ needs to be one-hot encoded. Simplest way to do this is to first convert the order-product pairs to a transaction list and then (as in Practical B) to a one-hot encoding.  \n",
       "\n",
       "~~~~.python\n",
       "# convert table of (order,product) pairs to list of transactions\n",
       "transactions = orders_products.groupby('order_id').apply(lambda order: order['product_id'].tolist())\n",
       "~~~~\n",
       "\n",
       "For each ARM with item (product, aisle, department) you should at least do the following subtasks:\n",
       "\n",
       " * Visualise the first 100 transactions vs items and comment (see Practical B).\n",
       " * Visualise the distribution of transaction size and comment (see Practical B).\n",
       " * Generate a few hundred frequent itemsets. You need to experiment (as discussed in class) to estimate a suitable value for the support threshold.\n",
       " * Generate a hundred+ rules and see if your can identify any interesting ones.\n",
       "\n",
       "#### Disclaimers/Comments\n",
       "\n",
       " * When using __aisle__ and __department__ for item in the ARM model, we really should be using more advanced models than just (0=absent, 1=present).  Feel free to explore the more advanced models, but we would be happy with just the one-hot analysis.\n",
       " \n",
       " * When you perform ARM with __item=product__ we will find the your rule list will swamped by the organic fanboy/girl customers (my term not Bernard's) and I realy hope those customers buying the $\\{lemon, lime\\}$ itemset were doing it for non-healthy reasons like drinking gin! \n",
       " \n",
       " * A more sophisticated analysis with __item=product__ would be to roll up (merge) essentially similar products. So \n",
       "   * All the organic fruit (${Organic\\ Strawberries, Organic\\ Raspberries, Bag\\ of\\ Organic\\ Bananas, \\ldots looong\\ list\\ldots }$) would just go to \"organic fruit\"\n",
       "   * Merge products which differ only by brand name. \n",
       "   \n",
       "#### Grading Outline\n",
       "\n",
       " * 3 x 25% for each of the separate basic ARM with item = (product, aisle, department) where the 25% consists of  \n",
       "   * 10% Visualisation \n",
       "   * 5% Rule generation \n",
       "   * 10% Comments/analysis and identification of interesting rules.\n",
       " * 25% for performing some roll-up of products and a more sophisticated analysis.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       " \n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Task - Linear regression \n",
       "\n",
       "#### Aim\n",
       "\n",
       "Perform a linear regression analysis (of your slice) of the Instacart dataset,\n",
       "predicting the number of days between orders.\n",
       "\n",
       "#### Detail\n",
       "\n",
       "The problem to solve takes the form of _Linear Regression with categorical\n",
       "independent variables_.\n",
       "\n",
       "Consider the sequence of orders placed by a customer. The hypothesis is that\n",
       "the typical product mix for that customer can be used to predict when the next\n",
       "order will be placed.\n",
       "\n",
       "For training data, take an 80% subset of \"relevant\" __order__s, across all\n",
       "\"relevant\" __user__s. From the first to the second last order, determine the\n",
       "__product__ mix for that __user__, as well as the __number of days__ until the\n",
       "next __order__ for that __user__.\n",
       "\n",
       "The training data is the remaining 20% subset of \"relevant\" __order__s, across\n",
       "all \"relevant\" __user__s. \n",
       "\n",
       "In practice, _relevant_ implies an unbiased subset chosen to make the problem\n",
       "tractable with the computing resources available to each student.\n",
       "\n",
       "For _Independent variables_, the __product__ (mix) for __order__ `i-1` can be\n",
       "used, and the _Dependent variable_ is the __number of days__ between __order__\n",
       "`i-1` and __order__ `i` in that sequence for a given __user__.\n",
       "\n",
       "Therefore number of observations is the number of __user__s times the number of\n",
       "__order__s for each __user__.\n",
       "\n",
       "Since there are 49688 distinct products, the dimensionality of the problem is\n",
       "very high, so students need to take account of this using:\n",
       "\n",
       "1. roll up to __aisle__ (134 unique) or __department__ (21 unique) and build\n",
       "   model accordingly \n",
       "2. use PCA to reduce the dimensions (of __product__s, __aisle__s and/or\n",
       "   departments), and solve using the reduced-dimension model\n",
       "3. use regularisation and fit a constrained model.\n",
       "\n",
       "#### Disclaimers/Comments\n",
       "\n",
       "* It is possible to obtain either per-user predictions (by solving a separate\n",
       "  regression problem for each user) or a user-agnostic prediction (by merging\n",
       "  the data for many users). It would be interesting to try both and compare,\n",
       "  explaining what you find. \n",
       " \n",
       "* Much of this task relates to dimensionality reduction. In traditional BI,\n",
       "  this would be done using roll-ups (e.g., from __product__ to __aisle__), but\n",
       "  linear regression offers other options. The BI approach is easier to\n",
       "  interpret, because of the \"meaningful\" labels, but does it work as well? Give\n",
       "  reasons fior your answer.\n",
       "\n",
       "* Validating the results requires analysis of the outputs (and not just the\n",
       "predictions) of the linear regression model, e.g., R-squared and any other\n",
       "metrics you believe are helpful here.\n",
       "   \n",
       "#### Grading Outline\n",
       "\n",
       " * 3 x 25% for each of the 3 regression model x dimensionality reduction options above, where the 25% consists of  \n",
       "   * 5% Visualisation \n",
       "   * 10% formulation and solving \n",
       "   * 10% Validation/Comments/analysis.\n",
       " * 25% for comparing per-user and multi-user models, and different methods for dimensionality reduction.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Task - Classification \n",
       "\n",
       "#### Aim\n",
       "\n",
       "Multi-label classification with numerical variables (of your slice) of the\n",
       "Instacart dataset, predicting the product mix in the next order for a user.\n",
       "\n",
       "#### Detail\n",
       "\n",
       "The problem to solve takes the form of _Classification with numerical\n",
       "independent variables_.\n",
       "\n",
       "Given the history of __order__s per user, Instacart would like to predict the\n",
       "mix of __product__s that would be ordered next. From a business perspective,\n",
       "there is little to be gained from promoting a product that would be purchased\n",
       "in any case, but much to be gained from promoting a related product. Therefore,\n",
       "knowing the baseline (i.e., without marketing intervention) could improve the\n",
       "efficiency of product marketing efforts.\n",
       "\n",
       "As with the regression task, we recommend that you choose a _relevant_\n",
       "subset of users for analysis, where _relevant_ implies an unbiased subset\n",
       "chosen to make the problem tractable with the computing resources available to\n",
       "each student.\n",
       "\n",
       "There are two subtasks that need to be considered:\n",
       "1. taking historical order data from (a subset of) users and using this data\n",
       "to predict the next order for a typical user, given that user's order history\n",
       "2. taking historical order data from a _single_ user and using this data to\n",
       "predict the next order for that user.\n",
       "\n",
       "The basic hypothesis in the first of these is that there is an underlying\n",
       "consistency across users in terms of their order behaviour, so the more data\n",
       "that is available, the better the prediction of a typical user's next order.\n",
       "\n",
       "By contrast, the hypothesis in the second of these is that the order behaviour\n",
       "of different differs from each other, so it is better to tailor next order\n",
       "predictions to individual users.\n",
       "\n",
       "We recommend the following three features for each __product__:\n",
       "\n",
       "1. relative quantity across all orders for that user (computed as the number of\n",
       "times it is ordered divided by the number of all products ordered)\n",
       "2. relative frequency of reorder (so if ordered twice in 4 orders, the reorder\n",
       "rate is (2-1)/(4-1) = 1/3). If not reordered, it takes the value 0.\n",
       "3. (some function of) \"add to cart order\", across all orders for a given user. For example, if it\n",
       "was ordered 3 times, and was the first, third and fifth item added to these orders,\n",
       "the corresponding feature would take the _mean_ value. Optionally, students might wish to investigate\n",
       "different ways of calculating this mean: the traditional\n",
       "[arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean) (3 in this case) or the\n",
       "[harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) (approximately 1.96 in this case)\n",
       "that gives more weight to smaller values.\n",
       "\n",
       "The 3 features above are chosen in an attempt to capture the following predictive attributes:\n",
       "* how often a product is ordered, compared to all other products for that user\n",
       "* how frequently a product is reordered by a given user\n",
       "* using \"add to cart order\" as a proxy to represent the importance of the product to the user\n",
       "\n",
       "Regarding feature 3, Kieran analysed the reorder rate as a function of \"add to cart order\"\n",
       "and found that it declined steadily as the \"add to cart order\" increased. This trend\n",
       "stopped abruptly at \"add to cart order\" 35 (approximately) when the relationship broke down,\n",
       "by exhibiting large oscillations.\n",
       "\n",
       "Therefore, we recommend that you, as part of the Exploratory Data Analysis phase, plot\n",
       "reorder rate (across all _relevant_ products and users) against \"add to cart order\", and\n",
       "use this to determine when the simple relationship breaks down, say at n=35.\n",
       "\n",
       "You can then use linear regression to fit a low-order polynomial (linear or quadratic, say)\n",
       "to reorder rate as a function of \"add to cart order\" for the restricted range while the\n",
       "relationship shows a stable trend. Using the linear regression you learned, you should then\n",
       "be able to extrapolate a reorder rate for higher \"add to cart order\" values.\n",
       "\n",
       "Indeed, it should be possible to use the results of this linear regression (and\n",
       "its extrapolation to higher \"add to cart order\" values) to\n",
       "lookup a generic reorder rate for a given (mean) \"add to cart order\" for a given\n",
       "__product__ for a given __user__. This generic reorder rate can then be used in\n",
       "place of feature 3 above, as it is derived from the same data and represents\n",
       "much the same behaviour.\n",
       "\n",
       "The _dependent_ variable is the product mix in the next order for that user in the\n",
       "training set, expressed as a set of binary variables (0,1-valued: product is included or not).\n",
       "\n",
       "The number of __product__s could cause difficulties, because the number of\n",
       "features (hence columns in the observation matrix) is 3 times the number of __product__s\n",
       "considered in the model, and the dependent variable is vector-valued, comprising\n",
       "indicator values for each of those products.\n",
       "\n",
       "Therefore, you are recommended to limit your analysis as follows:\n",
       "1. with multiple __user__s, take the most popular 200 products, say\n",
       "2. with a single __user__, limit to the __product__s purchased by that user in their\n",
       "orders to date.\n",
       "\n",
       "Regarding training, test data and validation, we recommend the following:\n",
       "\n",
       "1. given order data from multiple users, the training data comprises all orders\n",
       "for an 80% subset of the _relevant_ users. The learned classifier can then\n",
       "be applied to order data from the remaining 20% subset of users. For\n",
       "validation, you need to measure how well the classifer predicts the last order\n",
       "for those \"test\" users.\n",
       "2. given order data from a single user, the training data for that user excludes the\n",
       "last order for that user. For testing, the learned classifier is\n",
       "applied to all the data including the last order and the __product__ mix\n",
       "of the last order is predicted. For validation, the predicted last order\n",
       "can be compared with the actual last order for that user.\n",
       "\n",
       "#### Disclaimers/Comments\n",
       "\n",
       "* Feature engineering is a very important part of this task, because Instacart does\n",
       "not provide labels as part of the data. Therefore, they need to be derived.\n",
       " \n",
       "* The classifiers covered in this module generally do not support as many options\n",
       "for dimensionality reduction as linear regression does, so depending on the scope\n",
       "of the learned classifier, we recommend either a user-specific product subset, or\n",
       "a \"Top N\" approach (of products purchased by the _relevant_ users) in this task.\n",
       "Needless to say, it is essential to ensure that the same subset of __product__s\n",
       "is used throughout the task.\n",
       "\n",
       "* One of the objectives in this task is to determine whether per-user or\n",
       "across-user models make better predictions. Therefore, you will need to\n",
       "compare the two model scopes and suggest which is more applicable to the\n",
       "Instacart data.\n",
       "\n",
       "* Multilabel classification, as used here, is most commonly used in document\n",
       "classification. For example, we might wish to label a book with multiple non-exclusive\n",
       "attributes, such as its theme, its genre, its intended audience, and whether it\n",
       "belongs to a series or not. When used for market basket analysis, the main difficulty\n",
       "with multilabel classification is that the number of such labels (identifying products\n",
       "uniquely) can be enormous.\n",
       "\n",
       "#### Grading Outline\n",
       "\n",
       " * 25% deriving features 1 and 2 (will require data manipulation)\n",
       " * 25% deriving feature 3 (will require solving a linear regression sub-problem)\n",
       " * 20% choosing and using a classifier\n",
       " * 30% validating the results, to include analysis of confusion matrix, precision, recall and between-model metrics such as F1-score etc.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in [\"EDA\", \"ARM\", \"Regression\", \"Classification\"]:\n",
    "    content = open(('specification_%s.md' % task).replace(\" \",\"_\")).read()\n",
    "    printmd(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting your attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend that you submit 4 jupyter notebooks, each addressing the main tasks above.\n",
    "\n",
    "Your notebook should be fully annotated: you should explain any code and interpret any results. Even if a particular line of analysis does not seem to work for you, please explain what you did and why you believe it is not working. Marks will be given for any reasonable attempt - we cannot assign marks if you do not provide your \"workings\" at least!\n",
    "\n",
    "The tasks have different weightings, reflecting their different levels of difficulty and the time needed to complete them:\n",
    "\n",
    "   * EDA.     (20%)\n",
    "   * Model to predict days to next order (25%)\n",
    "   * ARM.    (15%)\n",
    "   * Classification to predict next order product mix (40%)\n",
    "\n",
    "Note that some EDA will be needed in support of the other 3 tasks, so you might choose to move your EDA efforts into the relevant task notebooks, but you should still indicate (via section headings and other annotations) what parts of those notebooks relate to EDA.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
